{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0ea264",
   "metadata": {},
   "source": [
    "# PART I. Sub Code"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "2a8ac237",
   "metadata": {},
   "source": [
    "### Anomaly Detector\n",
    "![image.png](attachment:image.png)\n",
    "e : error\n",
    "\n",
    "μ : mean\n",
    "\n",
    "Σ : positive-semidefinite and symmetric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8266e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def fit_norm_distribution_param(args, model, train_dataset, channel_idx=0):\n",
    "    predictions = []\n",
    "    organized = []\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        # Turn on evaluation mode which disables dropout.\n",
    "        model.eval()\n",
    "        pasthidden = model.init_hidden(1)\n",
    "        for t in range(len(train_dataset)):\n",
    "            out, hidden = model.forward(train_dataset[t].unsqueeze(0), pasthidden)\n",
    "            predictions.append([])\n",
    "            organized.append([])\n",
    "            errors.append([])\n",
    "            predictions[t].append(out.data.cpu()[0][0][channel_idx])\n",
    "            pasthidden = model.repackage_hidden(hidden)\n",
    "            for prediction_step in range(1,args.prediction_window_size):\n",
    "                out, hidden = model.forward(out, hidden)\n",
    "                predictions[t].append(out.data.cpu()[0][0][channel_idx])\n",
    "\n",
    "            if t >= args.prediction_window_size:\n",
    "                for step in range(args.prediction_window_size):\n",
    "                    organized[t].append(predictions[step+t-args.prediction_window_size][args.prediction_window_size-1-step])\n",
    "                organized[t]= torch.FloatTensor(organized[t]).to(args.device)\n",
    "                errors[t] = organized[t] - train_dataset[t][0][channel_idx]\n",
    "                errors[t] = errors[t].unsqueeze(0)\n",
    "\n",
    "    errors_tensor = torch.cat(errors[args.prediction_window_size:],dim=0)\n",
    "    mean = errors_tensor.mean(dim=0)\n",
    "    cov = errors_tensor.t().mm(errors_tensor)/errors_tensor.size(0) - mean.unsqueeze(1).mm(mean.unsqueeze(0))\n",
    "    # cov: positive-semidefinite and symmetric.\n",
    "\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def anomalyScore(args, model, dataset, mean, cov, channel_idx=0, score_predictor=None):\n",
    "    predictions = []\n",
    "    rearranged = []\n",
    "    errors = []\n",
    "    hiddens = []\n",
    "    predicted_scores = []\n",
    "    with torch.no_grad():\n",
    "        # Turn on evaluation mode which disables dropout.\n",
    "        model.eval()\n",
    "        pasthidden = model.init_hidden(1)\n",
    "        for t in range(len(dataset)):\n",
    "            out, hidden = model.forward(dataset[t].unsqueeze(0), pasthidden)\n",
    "            predictions.append([])\n",
    "            rearranged.append([])\n",
    "            errors.append([])\n",
    "            hiddens.append(model.extract_hidden(hidden))\n",
    "            if score_predictor is not None:\n",
    "                predicted_scores.append(score_predictor.predict(model.extract_hidden(hidden).numpy()))\n",
    "\n",
    "            predictions[t].append(out.data.cpu()[0][0][channel_idx])\n",
    "            pasthidden = model.repackage_hidden(hidden)\n",
    "            for prediction_step in range(1, args.prediction_window_size):\n",
    "                out, hidden = model.forward(out, hidden)\n",
    "                predictions[t].append(out.data.cpu()[0][0][channel_idx])\n",
    "\n",
    "            if t >= args.prediction_window_size:\n",
    "                for step in range(args.prediction_window_size):\n",
    "                    rearranged[t].append(\n",
    "                        predictions[step + t - args.prediction_window_size][args.prediction_window_size - 1 - step])\n",
    "                rearranged[t] =torch.FloatTensor(rearranged[t]).to(args.device).unsqueeze(0)\n",
    "                errors[t] = rearranged[t] - dataset[t][0][channel_idx]\n",
    "            else:\n",
    "                rearranged[t] = torch.zeros(1,args.prediction_window_size).to(args.device)\n",
    "                errors[t] = torch.zeros(1, args.prediction_window_size).to(args.device)\n",
    "\n",
    "    predicted_scores = np.array(predicted_scores)\n",
    "    scores = []\n",
    "    for error in errors:\n",
    "        mult1 = error-mean.unsqueeze(0) # [ 1 * prediction_window_size ]\n",
    "        mult2 = torch.inverse(cov) # [ prediction_window_size * prediction_window_size ]\n",
    "        mult3 = mult1.t() # [ prediction_window_size * 1 ]\n",
    "        score = torch.mm(mult1,torch.mm(mult2,mult3))\n",
    "        scores.append(score[0][0])\n",
    "\n",
    "    scores = torch.stack(scores)\n",
    "    rearranged = torch.cat(rearranged,dim=0)\n",
    "    errors = torch.cat(errors,dim=0)\n",
    "\n",
    "    return scores, rearranged, errors, hiddens, predicted_scores\n",
    "\n",
    "\n",
    "def get_precision_recall(args, score, label, num_samples, beta=1.0, sampling='log', predicted_score=None):\n",
    "    '''\n",
    "    :param args:\n",
    "    :param score: anomaly scores\n",
    "    :param label: anomaly labels\n",
    "    :param num_samples: the number of threshold samples\n",
    "    :param beta:\n",
    "    :param scale:\n",
    "    :return:\n",
    "    '''\n",
    "    if predicted_score is not None:\n",
    "        score = score - torch.FloatTensor(predicted_score).squeeze().to(args.device)\n",
    "\n",
    "    maximum = score.max()\n",
    "    if sampling=='log':\n",
    "        # Sample thresholds logarithmically\n",
    "        # The sampled thresholds are logarithmically spaced between: math:`10 ^ {start}` and: math:`10 ^ {end}`.\n",
    "        th = torch.logspace(0, torch.log10(torch.tensor(maximum)), num_samples).to(args.device)\n",
    "    else:\n",
    "        # Sample thresholds equally\n",
    "        # The sampled thresholds are equally spaced points between: attr:`start` and: attr:`end`\n",
    "        th = torch.linspace(0, maximum, num_samples).to(args.device)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for i in range(len(th)):\n",
    "        anomaly = (score > th[i]).float()\n",
    "        idx = anomaly * 2 + label\n",
    "        tn = (idx == 0.0).sum().item()  # tn\n",
    "        fn = (idx == 1.0).sum().item()  # fn\n",
    "        fp = (idx == 2.0).sum().item()  # fp\n",
    "        tp = (idx == 3.0).sum().item()  # tp\n",
    "\n",
    "        p = tp / (tp + fp + 1e-7)\n",
    "        r = tp / (tp + fn + 1e-7)\n",
    "\n",
    "        if p != 0 and r != 0:\n",
    "            precision.append(p)\n",
    "            recall.append(r)\n",
    "\n",
    "    precision = torch.FloatTensor(precision)\n",
    "    recall = torch.FloatTensor(recall)\n",
    "\n",
    "\n",
    "    f1 = (1 + beta ** 2) * (precision * recall).div(beta ** 2 * precision + recall + 1e-7)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db513ab",
   "metadata": {},
   "source": [
    "# PART II. Train Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380cd01",
   "metadata": {},
   "source": [
    "### 導入Tools & Args\n",
    "模型的各項參數從Args裡面做調整設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import preprocess_data\n",
    "from model import model\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from anomalyDetector import fit_norm_distribution_param\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch RNN Prediction Model on Time-series Dataset')\n",
    "parser.add_argument('--data', type=str, default='ofdm',\n",
    "                    help='type of the dataset (ecg, gesture, power_demand, space_shuttle, respiration, nyc_taxi, ofdm')\n",
    "parser.add_argument('--filename', type=str, default='NoiseSymbol.pkl',\n",
    "                    help='filename of the dataset')\n",
    "parser.add_argument('--model', type=str, default='GRU',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)')\n",
    "parser.add_argument('--augment', type=bool, default=True,\n",
    "                    help='augment')\n",
    "parser.add_argument('--emsize', type=int, default=32,\n",
    "                    help='size of rnn input features')\n",
    "parser.add_argument('--nhid', type=int, default=32,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--res_connection', action='store_true',\n",
    "                    help='residual connection')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, #0.0002\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=20000,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=64, metavar='N',\n",
    "                    help='eval_batch size')\n",
    "parser.add_argument('--bptt', type=int, default=10,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--teacher_forcing_ratio', type=float, default=0.7,\n",
    "                    help='teacher forcing ratio (deprecated)')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights (deprecated)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--device', type=str, default='cuda',\n",
    "                    help='cuda or cpu')\n",
    "parser.add_argument('--log_interval', type=int, default=10, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save_interval', type=int, default=10, metavar='N',\n",
    "                    help='save interval')\n",
    "parser.add_argument('--save_fig','-s', action='store_true',\n",
    "                    help='save figure')\n",
    "parser.add_argument('--resume','-r',\n",
    "                    help='use checkpoint model parameters as initial parameters (default: False)',\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('--pretrained','-p',\n",
    "                    help='use checkpoint model parameters and do not train anymore (default: False)',\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('--prediction_window_size', type=int, default=10,\n",
    "                    help='prediction_window_size')\n",
    "args = parser.parse_args()\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1ab3f",
   "metadata": {},
   "source": [
    "### 讀取Data & 建構Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b77770",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeseriesData = preprocess_data.PickleDataLoad(data_type=args.data, filename=args.filename,\n",
    "                                                augment_test_data=args.augment)\n",
    "train_dataset = TimeseriesData.batchify(args,TimeseriesData.trainData, args.batch_size)\n",
    "test_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, args.eval_batch_size)\n",
    "gen_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, 1)\n",
    "\n",
    "\n",
    "feature_dim = TimeseriesData.trainData.size(1)\n",
    "model = model.RNNPredictor(rnn_type = args.model,\n",
    "                           enc_inp_size=feature_dim,\n",
    "                           rnn_inp_size = args.emsize,\n",
    "                           rnn_hid_size = args.nhid,\n",
    "                           dec_out_size=feature_dim,\n",
    "                           nlayers = args.nlayers,\n",
    "                           dropout = args.dropout,\n",
    "                           tie_weights= args.tied,\n",
    "                           res_connection=args.res_connection).to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr= args.lr,weight_decay=args.weight_decay)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba7ce3",
   "metadata": {},
   "source": [
    "### 取得Batch size ＆ 生成Testing結果圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804a8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(args,source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len] # [ seq_len * batch_size * feature_size ]\n",
    "    target = source[i+1:i+1+seq_len] # [ (seq_len x batch_size x feature_size) ]\n",
    "    return data, target\n",
    "\n",
    "def generate_output(args,epoch, model, gen_dataset, disp_uncertainty=True,startPoint=500, endPoint=3500):\n",
    "    if args.save_fig:\n",
    "        # Turn on evaluation mode which disables dropout.\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden(1)\n",
    "        outSeq = []\n",
    "        upperlim95 = []\n",
    "        lowerlim95 = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(endPoint):\n",
    "                if i>=startPoint:\n",
    "                    # if disp_uncertainty and epoch > 40:\n",
    "                    #     outs = []\n",
    "                    #     model.train()\n",
    "                    #     for i in range(20):\n",
    "                    #         out_, hidden_ = model.forward(out+0.01*Variable(torch.randn(out.size())).cuda(),hidden,noise=True)\n",
    "                    #         outs.append(out_)\n",
    "                    #     model.eval()\n",
    "                    #     outs = torch.cat(outs,dim=0)\n",
    "                    #     out_mean = torch.mean(outs,dim=0) # [bsz * feature_dim]\n",
    "                    #     out_std = torch.std(outs,dim=0) # [bsz * feature_dim]\n",
    "                    #     upperlim95.append(out_mean + 2.58*out_std/np.sqrt(20))\n",
    "                    #     lowerlim95.append(out_mean - 2.58*out_std/np.sqrt(20))\n",
    "\n",
    "                    out, hidden = model.forward(out, hidden)\n",
    "\n",
    "                    #print(out_mean,out)\n",
    "\n",
    "                else:\n",
    "                    out, hidden = model.forward(gen_dataset[i].unsqueeze(0), hidden)\n",
    "                outSeq.append(out.data.cpu()[0][0].unsqueeze(0))\n",
    "\n",
    "\n",
    "        outSeq = torch.cat(outSeq,dim=0) # [seqLength * feature_dim]\n",
    "\n",
    "        target= preprocess_data.reconstruct(gen_dataset.cpu(), TimeseriesData.mean, TimeseriesData.std)\n",
    "        outSeq = preprocess_data.reconstruct(outSeq, TimeseriesData.mean, TimeseriesData.std)\n",
    "        # if epoch>40:\n",
    "        #     upperlim95 = torch.cat(upperlim95, dim=0)\n",
    "        #     lowerlim95 = torch.cat(lowerlim95, dim=0)\n",
    "        #     upperlim95 = preprocess_data.reconstruct(upperlim95.data.cpu().numpy(),TimeseriesData.mean,TimeseriesData.std)\n",
    "        #     lowerlim95 = preprocess_data.reconstruct(lowerlim95.data.cpu().numpy(),TimeseriesData.mean,TimeseriesData.std)\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        for i in range(target.size(-1)):\n",
    "            plt.plot(target[:,:,i].numpy(), label='Target'+str(i),\n",
    "                     color='black', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            plt.plot(range(startPoint), outSeq[:startPoint,i].numpy(), label='1-step predictions for target'+str(i),\n",
    "                     color='green', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "            # if epoch>40:\n",
    "            #     plt.plot(range(startPoint, endPoint), upperlim95[:,i].numpy(), label='upperlim'+str(i),\n",
    "            #              color='skyblue', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "            #     plt.plot(range(startPoint, endPoint), lowerlim95[:,i].numpy(), label='lowerlim'+str(i),\n",
    "            #              color='skyblue', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "            plt.plot(range(startPoint, endPoint), outSeq[startPoint:,i].numpy(), label='Recursive predictions for target'+str(i),\n",
    "                     color='blue', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "\n",
    "        plt.xlim([startPoint-500, endPoint])\n",
    "        plt.xlabel('Index',fontsize=15)\n",
    "        plt.ylabel('Value',fontsize=15)\n",
    "        plt.title('Time-series Prediction on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.text(startPoint-500+10, target.min(), 'Epoch: '+str(epoch),fontsize=15)\n",
    "        save_dir = Path('result',args.data,args.filename).with_suffix('').joinpath('fig_prediction')\n",
    "        save_dir.mkdir(parents=True,exist_ok=True)\n",
    "        plt.savefig(save_dir.joinpath('fig_epoch'+str(epoch)).with_suffix('.png'))\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        return outSeq\n",
    "\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd986dd5",
   "metadata": {},
   "source": [
    "### Training code \n",
    "如果dropout為0則為evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_dataset,epoch):\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Turn on training mode which enables dropout.\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(args.batch_size)\n",
    "        for batch, i in enumerate(range(0, train_dataset.size(0) - 1, args.bptt)):\n",
    "            inputSeq, targetSeq = get_batch(args,train_dataset, i)\n",
    "            # inputSeq: [ seq_len * batch_size * feature_size ]\n",
    "            # targetSeq: [ seq_len * batch_size * feature_size ]\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = model.repackage_hidden(hidden)\n",
    "            hidden_ = model.repackage_hidden(hidden)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            '''Loss1: Free running loss'''\n",
    "            outVal = inputSeq[0].unsqueeze(0)\n",
    "            outVals=[]\n",
    "            hids1 = []\n",
    "            for i in range(inputSeq.size(0)):\n",
    "                outVal, hidden_, hid = model.forward(outVal, hidden_,return_hiddens=True)\n",
    "                outVals.append(outVal)\n",
    "                hids1.append(hid)\n",
    "            outSeq1 = torch.cat(outVals,dim=0)\n",
    "            hids1 = torch.cat(hids1,dim=0)\n",
    "            loss1 = criterion(outSeq1.contiguous().view(args.batch_size,-1), targetSeq.contiguous().view(args.batch_size,-1))\n",
    "\n",
    "            '''Loss2: Teacher forcing loss'''\n",
    "            outSeq2, hidden, hids2 = model.forward(inputSeq, hidden, return_hiddens=True)\n",
    "            loss2 = criterion(outSeq2.contiguous().view(args.batch_size, -1), targetSeq.contiguous().view(args.batch_size, -1))\n",
    "\n",
    "            '''Loss3: Simplified Professor forcing loss'''\n",
    "            loss3 = criterion(hids1.contiguous().view(args.batch_size,-1), hids2.contiguous().view(args.batch_size,-1).detach())\n",
    "\n",
    "            '''Total loss = Loss1+Loss2+Loss3'''\n",
    "            loss = loss1+loss2+loss3\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch % args.log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / args.log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.4f} | '\n",
    "                      'loss {:5.2f} '.format(\n",
    "                    epoch, batch, len(train_dataset) // args.bptt,\n",
    "                                  elapsed * 1000 / args.log_interval, cur_loss))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd78ab",
   "metadata": {},
   "source": [
    "### Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c51965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, test_dataset):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        hidden = model.init_hidden(args.eval_batch_size)\n",
    "        nbatch = 1\n",
    "        for nbatch, i in enumerate(range(0, test_dataset.size(0) - 1, args.bptt)):\n",
    "            inputSeq, targetSeq = get_batch(args,test_dataset, i)\n",
    "            # inputSeq: [ seq_len * batch_size * feature_size ]\n",
    "            # targetSeq: [ seq_len * batch_size * feature_size ]\n",
    "            hidden_ = model.repackage_hidden(hidden)\n",
    "            '''Loss1: Free running loss'''\n",
    "            outVal = inputSeq[0].unsqueeze(0)\n",
    "            outVals=[]\n",
    "            hids1 = []\n",
    "            for i in range(inputSeq.size(0)):\n",
    "                outVal, hidden_, hid = model.forward(outVal, hidden_,return_hiddens=True)\n",
    "                outVals.append(outVal)\n",
    "                hids1.append(hid)\n",
    "            outSeq1 = torch.cat(outVals,dim=0)\n",
    "            hids1 = torch.cat(hids1,dim=0)\n",
    "            loss1 = criterion(outSeq1.contiguous().view(args.batch_size,-1), targetSeq.contiguous().view(args.batch_size,-1))\n",
    "\n",
    "            '''Loss2: Teacher forcing loss'''\n",
    "            outSeq2, hidden, hids2 = model.forward(inputSeq, hidden, return_hiddens=True)\n",
    "            loss2 = criterion(outSeq2.contiguous().view(args.batch_size, -1), targetSeq.contiguous().view(args.batch_size, -1))\n",
    "\n",
    "            '''Loss3: Simplified Professor forcing loss'''\n",
    "            loss3 = criterion(hids1.view(args.batch_size,-1), hids2.view(args.batch_size,-1).detach())\n",
    "\n",
    "            '''Total loss = Loss1+Loss2+Loss3'''\n",
    "            loss = loss1+loss2+loss3\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (nbatch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bde71d",
   "metadata": {},
   "source": [
    "### Main code\n",
    "每當training十次epoch後，儲存一次checkpoint，讓我們可以提早結束training，\n",
    "\n",
    "如需繼續training可以從checkpoint處繼續，或是透過checkpoint將pretrained的model讀取出來進行參數調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00827fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "if args.resume or args.pretrained:\n",
    "    print(\"=> loading checkpoint \")\n",
    "    checkpoint = torch.load(Path('save', args.data, 'checkpoint', args.filename).with_suffix('.pth'))\n",
    "    args, start_epoch, best_val_loss = model.load_checkpoint(args,checkpoint,feature_dim)\n",
    "    optimizer.load_state_dict((checkpoint['optimizer']))\n",
    "    del checkpoint\n",
    "    epoch = start_epoch\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    epoch = 1\n",
    "    start_epoch = 1\n",
    "    best_val_loss = float('inf')\n",
    "    print(\"=> Start training from scratch\")\n",
    "print('-' * 89)\n",
    "print(args)\n",
    "print('-' * 89)\n",
    "\n",
    "if not args.pretrained:\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        for epoch in range(start_epoch, args.epochs+1):\n",
    "\n",
    "            epoch_start_time = time.time()\n",
    "            train(args,model,train_dataset,epoch)\n",
    "            val_loss = evaluate(args,model,test_dataset)\n",
    "            print('-' * 89)\n",
    "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.4f} | '.format(epoch, (time.time() - epoch_start_time), val_loss))\n",
    "            print('-' * 89)\n",
    "\n",
    "            generate_output(args,epoch,model,gen_dataset,startPoint=1500)\n",
    "\n",
    "            if epoch%args.save_interval==0:\n",
    "                # Save the model if the validation loss is the best we've seen so far.\n",
    "                is_best = val_loss < best_val_loss\n",
    "                best_val_loss = min(val_loss, best_val_loss)\n",
    "                model_dictionary = {'epoch': epoch,\n",
    "                                    'best_loss': best_val_loss,\n",
    "                                    'state_dict': model.state_dict(),\n",
    "                                    'optimizer': optimizer.state_dict(),\n",
    "                                    'args':args\n",
    "                                    }\n",
    "                model.save_checkpoint(model_dictionary, is_best)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "\n",
    "\n",
    "# Calculate mean and covariance for each channel's prediction errors, and save them with the trained model\n",
    "print('=> calculating mean and covariance')\n",
    "means, covs = list(),list()\n",
    "train_dataset = TimeseriesData.batchify(args, TimeseriesData.trainData, bsz=1)\n",
    "for channel_idx in range(model.enc_input_size):\n",
    "    mean, cov = fit_norm_distribution_param(args,model,train_dataset[:TimeseriesData.length],channel_idx)\n",
    "    means.append(mean), covs.append(cov)\n",
    "model_dictionary = {'epoch': max(epoch,start_epoch),\n",
    "                    'best_loss': best_val_loss,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'args': args,\n",
    "                    'means': means,\n",
    "                    'covs': covs\n",
    "                    }\n",
    "model.save_checkpoint(model_dictionary, True)\n",
    "print('-' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be977e82",
   "metadata": {},
   "source": [
    "# PART III. Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d0776",
   "metadata": {},
   "source": [
    "### 導入Tools & Args\n",
    "模型的各項參數從Args裡面做調整設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pickle\n",
    "import preprocess_data\n",
    "from model import model\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from anomalyDetector import fit_norm_distribution_param\n",
    "from anomalyDetector import anomalyScore\n",
    "from anomalyDetector import get_precision_recall\n",
    "parser = argparse.ArgumentParser(description='PyTorch RNN Anomaly Detection Model')\n",
    "parser.add_argument('--prediction_window_size', type=int, default=10,\n",
    "                    help='prediction_window_size')\n",
    "parser.add_argument('--data', type=str, default='ofdm',\n",
    "                    help='type of the dataset (ecg, gesture, power_demand, space_shuttle, respiration, nyc_taxi, ofdm')\n",
    "parser.add_argument('--filename', type=str, default='NoiseSymbol.pkl',\n",
    "                    help='filename of the dataset')\n",
    "parser.add_argument('--save_fig','-s', action='store_true',\n",
    "                    help='save results as figures')\n",
    "parser.add_argument('--compensate', action='store_true',\n",
    "                    help='compensate anomaly score using anomaly score esimation')\n",
    "parser.add_argument('--beta', type=float, default=1.0,\n",
    "                    help='beta value for f-beta score')\n",
    "\n",
    "\n",
    "args_ = parser.parse_args()\n",
    "print('-' * 89)\n",
    "print(\"=> loading checkpoint \")\n",
    "checkpoint = torch.load(str(Path('save',args_.data,'checkpoint',args_.filename).with_suffix('.pth')))\n",
    "args = checkpoint['args']\n",
    "args.prediction_window_size= args_.prediction_window_size\n",
    "args.beta = args_.beta\n",
    "args.save_fig = args_.save_fig\n",
    "args.compensate = args_.compensate\n",
    "print(\"=> loaded checkpoint\")\n",
    "\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f3a4f",
   "metadata": {},
   "source": [
    "### 讀取Data & 建構Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000068a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeseriesData = preprocess_data.PickleDataLoad(data_type=args.data,filename=args.filename, augment_test_data=False)\n",
    "train_dataset = TimeseriesData.batchify(args,TimeseriesData.trainData[:TimeseriesData.length], bsz=1)\n",
    "test_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, bsz=1)\n",
    "\n",
    "\n",
    "nfeatures = TimeseriesData.trainData.size(-1)\n",
    "model = model.RNNPredictor(rnn_type = args.model,\n",
    "                           enc_inp_size=nfeatures,\n",
    "                           rnn_inp_size = args.emsize,\n",
    "                           rnn_hid_size = args.nhid,\n",
    "                           dec_out_size=nfeatures,\n",
    "                           nlayers = args.nlayers,\n",
    "                           res_connection=args.res_connection).to(args.device)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c9ead",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016df2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, predicted_scores, precisions, recalls, f_betas = list(), list(), list(), list(), list()\n",
    "targets, mean_predictions, oneStep_predictions, Nstep_predictions = list(), list(), list(), list()\n",
    "try:\n",
    "    # For each channel in the dataset\n",
    "    for channel_idx in range(nfeatures):\n",
    "        ''' 1. Load mean and covariance if they are pre-calculated, if not calculate them. '''\n",
    "        # Mean and covariance are calculated on train dataset.\n",
    "        if 'means' in checkpoint.keys() and 'covs' in checkpoint.keys():\n",
    "            print('=> loading pre-calculated mean and covariance')\n",
    "            mean, cov = checkpoint['means'][channel_idx], checkpoint['covs'][channel_idx]\n",
    "        else:\n",
    "            print('=> calculating mean and covariance')\n",
    "            mean, cov = fit_norm_distribution_param(args, model, train_dataset, channel_idx=channel_idx)\n",
    "\n",
    "        ''' 2. Train anomaly score predictor using support vector regression (SVR). (Optional) '''\n",
    "        # An anomaly score predictor is trained\n",
    "        # given hidden layer output and the corresponding anomaly score on train dataset.\n",
    "        # Predicted anomaly scores on test dataset can be used for the baseline of the adaptive threshold.\n",
    "        if args.compensate:\n",
    "            print('=> training an SVR as anomaly score predictor')\n",
    "            train_score, _, _, hiddens, _ = anomalyScore(args, model, train_dataset, mean, cov, channel_idx=channel_idx)\n",
    "            score_predictor = GridSearchCV(SVR(), cv=5,param_grid={\"C\": [1e0, 1e1, 1e2],\"gamma\": np.logspace(-1, 1, 3)})\n",
    "            score_predictor.fit(torch.cat(hiddens,dim=0).numpy(), train_score.cpu().numpy())\n",
    "        else:\n",
    "            score_predictor=None\n",
    "\n",
    "        ''' 3. Calculate anomaly scores'''\n",
    "        # Anomaly scores are calculated on the test dataset\n",
    "        # given the mean and the covariance calculated on the train dataset\n",
    "        print('=> calculating anomaly scores')\n",
    "        score, sorted_prediction, sorted_error, _, predicted_score = anomalyScore(args, model, test_dataset, mean, cov,\n",
    "                                                                                  score_predictor=score_predictor,\n",
    "                                                                                  channel_idx=channel_idx)\n",
    "\n",
    "        ''' 4. Evaluate the result '''\n",
    "        # The obtained anomaly scores are evaluated by measuring precision, recall, and f_beta scores\n",
    "        # The precision, recall, f_beta scores are are calculated repeatedly,\n",
    "        # sampling the threshold from 1 to the maximum anomaly score value, either equidistantly or logarithmically.\n",
    "        print('=> calculating precision, recall, and f_beta')\n",
    "        precision, recall, f_beta = get_precision_recall(args, score, num_samples=1000, beta=args.beta,\n",
    "                                                         label=TimeseriesData.testLabel.to(args.device))\n",
    "        print('data: ',args.data,' filename: ',args.filename,\n",
    "              ' f-beta (no compensation): ', f_beta.max().item(),' beta: ',args.beta)\n",
    "        if args.compensate:\n",
    "            precision, recall, f_beta = get_precision_recall(args, score, num_samples=1000, beta=args.beta,\n",
    "                                                             label=TimeseriesData.testLabel.to(args.device),\n",
    "                                                             predicted_score=predicted_score)\n",
    "            print('data: ',args.data,' filename: ',args.filename,\n",
    "                  ' f-beta    (compensation): ', f_beta.max().item(),' beta: ',args.beta)\n",
    "\n",
    "\n",
    "        target = preprocess_data.reconstruct(test_dataset.cpu()[:, 0, channel_idx],\n",
    "                                             TimeseriesData.mean[channel_idx],\n",
    "                                             TimeseriesData.std[channel_idx]).numpy()\n",
    "        mean_prediction = preprocess_data.reconstruct(sorted_prediction.mean(dim=1).cpu(),\n",
    "                                                      TimeseriesData.mean[channel_idx],\n",
    "                                                      TimeseriesData.std[channel_idx]).numpy()\n",
    "        oneStep_prediction = preprocess_data.reconstruct(sorted_prediction[:, -1].cpu(),\n",
    "                                                         TimeseriesData.mean[channel_idx],\n",
    "                                                         TimeseriesData.std[channel_idx]).numpy()\n",
    "        Nstep_prediction = preprocess_data.reconstruct(sorted_prediction[:, 0].cpu(),\n",
    "                                                       TimeseriesData.mean[channel_idx],\n",
    "                                                       TimeseriesData.std[channel_idx]).numpy()\n",
    "        sorted_errors_mean = sorted_error.abs().mean(dim=1).cpu()\n",
    "        sorted_errors_mean *= TimeseriesData.std[channel_idx]\n",
    "        sorted_errors_mean = sorted_errors_mean.numpy()\n",
    "        score = score.cpu()\n",
    "        scores.append(score), targets.append(target), predicted_scores.append(predicted_score)\n",
    "        mean_predictions.append(mean_prediction), oneStep_predictions.append(oneStep_prediction)\n",
    "        Nstep_predictions.append(Nstep_prediction)\n",
    "        precisions.append(precision), recalls.append(recall), f_betas.append(f_beta)\n",
    "\n",
    "\n",
    "        if args.save_fig:\n",
    "            save_dir = Path('result',args.data,args.filename).with_suffix('').joinpath('fig_detection')\n",
    "            save_dir.mkdir(parents=True,exist_ok=True)\n",
    "            plt.plot(precision.cpu().numpy(),label='precision')\n",
    "            plt.plot(recall.cpu().numpy(),label='recall')\n",
    "            plt.plot(f_beta.cpu().numpy(), label='f1')\n",
    "            plt.legend()\n",
    "            plt.xlabel('Threshold (log scale)')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title('Anomaly Detection on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "            plt.savefig(str(save_dir.joinpath('fig_f_beta_channel'+str(channel_idx)).with_suffix('.png')))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "            fig, ax1 = plt.subplots(figsize=(15,5))\n",
    "            ax1.plot(target,label='Target',\n",
    "                     color='black',  marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(mean_prediction, label='Mean predictions',\n",
    "                     color='purple', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(oneStep_prediction, label='1-step predictions',\n",
    "                     color='green', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(Nstep_prediction, label=str(args.prediction_window_size) + '-step predictions',\n",
    "                     color='blue', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(sorted_errors_mean,label='Absolute mean prediction errors',\n",
    "                     color='orange', marker='.', linestyle='--', markersize=1, linewidth=1.0)\n",
    "            ax1.legend(loc='upper left')\n",
    "            ax1.set_ylabel('Value',fontsize=15)\n",
    "            ax1.set_xlabel('Index',fontsize=15)\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(score.numpy().reshape(-1, 1), label='Anomaly scores from \\nmultivariate normal distribution',\n",
    "                     color='red', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "            if args.compensate:\n",
    "                ax2.plot(predicted_score, label='Predicted anomaly scores from SVR',\n",
    "                         color='cyan', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "            #ax2.plot(score.reshape(-1,1)/(predicted_score+1),label='Anomaly scores from \\nmultivariate normal distribution',\n",
    "            #        color='hotpink', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylabel('anomaly score',fontsize=15)\n",
    "            #plt.axvspan(2830,2900 , color='yellow', alpha=0.3)\n",
    "            plt.title('Anomaly Detection on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.xlim([0,len(test_dataset)])\n",
    "            plt.savefig(str(save_dir.joinpath('fig_scores_channel'+str(channel_idx)).with_suffix('.png')))\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "\n",
    "print('=> saving the results as pickle extensions')\n",
    "save_dir = Path('result', args.data, args.filename).with_suffix('')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "pickle.dump(targets, open(str(save_dir.joinpath('target.pkl')),'wb'))\n",
    "pickle.dump(mean_predictions, open(str(save_dir.joinpath('mean_predictions.pkl')),'wb'))\n",
    "pickle.dump(oneStep_predictions, open(str(save_dir.joinpath('oneStep_predictions.pkl')),'wb'))\n",
    "pickle.dump(Nstep_predictions, open(str(save_dir.joinpath('Nstep_predictions.pkl')),'wb'))\n",
    "pickle.dump(scores, open(str(save_dir.joinpath('score.pkl')),'wb'))\n",
    "pickle.dump(predicted_scores, open(str(save_dir.joinpath('predicted_scores.pkl')),'wb'))\n",
    "pickle.dump(precisions, open(str(save_dir.joinpath('precision.pkl')),'wb'))\n",
    "pickle.dump(recalls, open(str(save_dir.joinpath('recall.pkl')),'wb'))\n",
    "pickle.dump(f_betas, open(str(save_dir.joinpath('f_beta.pkl')),'wb'))\n",
    "print('-' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
