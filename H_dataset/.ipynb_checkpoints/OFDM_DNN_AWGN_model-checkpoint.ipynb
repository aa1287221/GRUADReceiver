{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # BE QUIET!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 64 # subcarriers = K\n",
    "CP = K//4\n",
    "P = 64 # number of pilot carriers per OFDM block\n",
    "#pilotValue = 1+1j\n",
    "allCarriers = np.arange(K)  # indices of all subcarriers ([0, 1, ... K-1])\n",
    "pilotCarriers = allCarriers[::K//P] # Pilots is every (K/P)th carrier.\n",
    "#pilotCarriers = np.hstack([pilotCarriers, np.array([allCarriers[-1]])])\n",
    "#P = P+1\n",
    "dataCarriers = np.delete(allCarriers, pilotCarriers)\n",
    "mu = 2    # one symbol combined with two bits for QAM or QPSK (LJS)\n",
    "payloadBits_per_OFDM = len(dataCarriers)*mu  # number of payload bits per OFDM symbol\n",
    "\n",
    "payloadBits_per_OFDM = K*mu # payloadbits per OFDM version 2 (decided by how many data carriers per OFDM , LJS)\n",
    "\n",
    "SNRdb = 20  # signal to noise-ratio in dB at the receiver \n",
    "\n",
    "mapping_table = {\n",
    "    (0,0) : -1-1j,\n",
    "    (0,1) : -1+1j,\n",
    "    (1,0) : 1-1j,\n",
    "    (1,1) : 1+1j,\n",
    "}\n",
    "\n",
    "demapping_table = {v : k for k, v in mapping_table.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Training Pilots txt\n"
     ]
    }
   ],
   "source": [
    "def Modulation(bits):                                        \n",
    "    bit_r = bits.reshape((int(len(bits)/mu), mu))                  \n",
    "    return (2*bit_r[:,0]-1)+1j*(2*bit_r[:,1]-1) # This is just for QAM modulation\n",
    "\n",
    "def OFDM_symbol(Data, pilot_flag):\n",
    "    symbol = np.zeros(K, dtype=complex) # the overall K subcarriers\n",
    "    #symbol = np.zeros(K) \n",
    "    symbol[pilotCarriers] = pilotValue  # allocate the pilot subcarriers \n",
    "    symbol[dataCarriers] = Data  # allocate the pilot subcarriers\n",
    "    return symbol\n",
    "\n",
    "def IDFT(OFDM_data):\n",
    "    return np.fft.ifft(OFDM_data)   # np.fft.ifft(OFDM_data)*np.sqrt(K)  (lJS)\n",
    "\n",
    "def addCP(OFDM_time):\n",
    "    cp = OFDM_time[-CP:] # take the last CP samples ...\n",
    "    return np.hstack([cp, OFDM_time])  # ... and add them to the beginning\n",
    "\n",
    "def channel(signal,channelResponse,SNRdb):       # construct the another version is including impulse noise(LJS)\n",
    "    # AWGN channel\n",
    "    convolved = np.convolve(signal, channelResponse)\n",
    "    signal_power = np.mean(abs(convolved**2))\n",
    "    sigma2 = signal_power * 10**(-SNRdb/10)  \n",
    "    noise = np.sqrt(sigma2/2) * (np.random.randn(*convolved.shape)+1j*np.random.randn(*convolved.shape))\n",
    "    \n",
    "    # Bernoulli-Gaussian channel          # lJS\n",
    "    # Impulse noise ver 1.0  ( Faster version )\n",
    "    prob=0.05  \n",
    "    convolved = np.convolve(signal, channelResponse)\n",
    "    signal_power = np.mean(abs(convolved**2))\n",
    "    sigma2 = signal_power * 10**(-SNRdb/10)      # (signal_power/2)  (LJS)\n",
    "    sigma3 = 2.5\n",
    "    Gaussian=np.random.randn(*convolved.shape)+1j*np.random.randn(*convolved.shape)\n",
    "    power1=np.zeros([*convolved.shape])\n",
    "    for i in range(*convolved.shape):\n",
    "        k=np.random.rand()\n",
    "        if k >  prob :\n",
    "            power1[i]=np.sqrt(sigma2/2)\n",
    "        if k <= prob :\n",
    "            power1[i]=np.sqrt(sigma3/2)\n",
    "    power2=np.zeros([*convolved.shape])\n",
    "    for i in range(*convolved.shape):\n",
    "        k=np.random.rand()\n",
    "        if k >  prob :\n",
    "            power2[i]=np.sqrt(sigma2/2)\n",
    "        if k <= prob :\n",
    "            power2[i]=np.sqrt(sigma3/2)\n",
    "    noise1=np.multiply(power1,Gaussian.real)\n",
    "    noise2=np.multiply(power2,Gaussian.imag)\n",
    "    noise_BG=np.zeros([*convolved.shape]).astype(complex)\n",
    "    noise_BG.real=noise1\n",
    "    noise_BG.imag=noise2\n",
    "    \n",
    "    # Impulse noise ver 2.0  ( Slower version )\n",
    "#     prob=0.01  \n",
    "#     convolved = np.convolve(signal, channelResponse)\n",
    "#     signal_power = np.mean(abs(convolved**2))\n",
    "#     sigma2 = signal_power * 10**(-SNRdb/10)      # (signal_power/2)  (LJS)\n",
    "#     sigma3 = 2\n",
    "#     noise_BG=np.zeros([*convolved.shape]).astype(complex)\n",
    "#     for i in range(*convolved.shape):\n",
    "#         k=np.random.rand()   \n",
    "#         if k > prob  :\n",
    "#             noise_BG[i] = np.sqrt(sigma2/2) * (np.random.randn(1)+1j*np.random.randn(1))\n",
    "#         if k <= prob :\n",
    "#             noise_BG[i] = np.sqrt(sigma3/2) * (np.random.randn(1)+1j*np.random.randn(1))\n",
    "# #             print('noise :',noise[i])\n",
    "# #     print(noise_BG)   \n",
    "    return convolved + noise    # convolved + noise_BG\n",
    "\n",
    "def removeCP(signal):\n",
    "    return signal[CP:(CP+K)]\n",
    "\n",
    "def equalize(OFDM_demod, Hest):   # *(LJS)\n",
    "    return OFDM_demod / Hest\n",
    "\n",
    "def get_payload(equalized):       # **(LJS)\n",
    "    return equalized[dataCarriers]\n",
    "\n",
    "def PS(bits):                     # *(LJS)\n",
    "    return bits.reshape((-1,))\n",
    "\n",
    "def clipper(OFDM_RX_noCP):         # LJS\n",
    "    clipper_threshold=0.15         # initial value = 0.1 : 0.05 : 0.5\n",
    "    norm=np.abs(OFDM_RX_noCP)\n",
    "    angle=np.angle(OFDM_RX_noCP)\n",
    "    for i in range(*OFDM_RX_noCP.shape):\n",
    "        if norm[i] >= clipper_threshold:\n",
    "            OFDM_RX_noCP.real[i]=clipper_threshold*np.cos(angle[i])\n",
    "            OFDM_RX_noCP.imag[i]=clipper_threshold*np.sin(angle[i])\n",
    "    return OFDM_RX_noCP\n",
    "\n",
    "def ofdm_simulate(codeword, channelResponse,SNRdb):       # LJS\n",
    "    OFDM_data = np.zeros(K, dtype=complex)\n",
    "    OFDM_data[allCarriers] = pilotValue\n",
    "    OFDM_time = IDFT(OFDM_data)\n",
    "    OFDM_withCP = addCP(OFDM_time)\n",
    "    OFDM_TX = OFDM_withCP\n",
    "    OFDM_RX = channel(OFDM_TX, channelResponse,SNRdb)\n",
    "    OFDM_RX_noCP = removeCP(OFDM_RX)\n",
    "#     plt.plot(np.abs(OFDM_RX_noCP))\n",
    "#     plt.show()\n",
    "    # ----- target inputs ---\n",
    "    symbol = np.zeros(K, dtype=complex)\n",
    "    codeword_qam = Modulation(codeword)\n",
    "    symbol[np.arange(K)] = codeword_qam\n",
    "    OFDM_data_codeword = symbol\n",
    "    OFDM_time_codeword = np.fft.ifft(OFDM_data_codeword)\n",
    "    OFDM_withCP_cordword = addCP(OFDM_time_codeword)\n",
    "    OFDM_RX_codeword = channel(OFDM_withCP_cordword, channelResponse,SNRdb)\n",
    "    OFDM_RX_noCP_codeword = removeCP(OFDM_RX_codeword)\n",
    "#     plt.plot(np.abs(OFDM_RX_noCP_codeword))\n",
    "#     plt.show()\n",
    "    return np.concatenate((np.concatenate((np.real(OFDM_RX_noCP),np.imag(OFDM_RX_noCP))), np.concatenate((np.real(OFDM_RX_noCP_codeword),np.imag(OFDM_RX_noCP_codeword))))), abs(channelResponse) \n",
    "\n",
    "def ofdm_simulate_clip(codeword, channelResponse,SNRdb):       # LJS\n",
    "    OFDM_data = np.zeros(K, dtype=complex)\n",
    "    OFDM_data[allCarriers] = pilotValue\n",
    "    OFDM_time = IDFT(OFDM_data)\n",
    "    OFDM_withCP = addCP(OFDM_time)\n",
    "    OFDM_TX = OFDM_withCP\n",
    "    OFDM_RX = channel(OFDM_TX, channelResponse,SNRdb)\n",
    "    OFDM_RX_noCP = removeCP(OFDM_RX)\n",
    "    OFDM_RX_noCP_clip = clipper(OFDM_RX_noCP)  # LJS\n",
    "    # ----- target inputs ---\n",
    "    symbol = np.zeros(K, dtype=complex)\n",
    "    codeword_qam = Modulation(codeword)\n",
    "    symbol[np.arange(K)] = codeword_qam\n",
    "    OFDM_data_codeword = symbol\n",
    "    OFDM_time_codeword = np.fft.ifft(OFDM_data_codeword)\n",
    "    OFDM_withCP_cordword = addCP(OFDM_time_codeword)\n",
    "    OFDM_RX_codeword = channel(OFDM_withCP_cordword, channelResponse,SNRdb)\n",
    "    OFDM_RX_noCP_codeword = removeCP(OFDM_RX_codeword)\n",
    "    OFDM_RX_noCP_codeword_clip = clipper(OFDM_RX_noCP_codeword)  # LJS\n",
    "    return np.concatenate((np.concatenate((np.real(OFDM_RX_noCP_clip),np.imag(OFDM_RX_noCP_clip))), np.concatenate((np.real(OFDM_RX_noCP_codeword_clip),np.imag(OFDM_RX_noCP_codeword_clip))))), abs(channelResponse) \n",
    "\n",
    "def decision(y_pred_np,batch_y):       # LJS\n",
    "    size=np.shape(batch_y)   \n",
    "    y_pred_trans=np.zeros([size[0],size[1]])\n",
    "    for k in range(size[0]):\n",
    "        for p in range(size[1]):\n",
    "            if y_pred_np[k,p] > 0.5 :\n",
    "                y_pred_trans[k,p] = 1\n",
    "            if y_pred_np[k,p] < 0.5 :\n",
    "                y_pred_trans[k,p] = 0\n",
    "            if y_pred_np[k,p] == 0.5 :\n",
    "                y_pred_trans[k,p] = np.random.randint(0,2)    \n",
    "    count=0\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            if y_pred_trans[i,j] != batch_y[i,j]:\n",
    "                count=count+1                            \n",
    "    BER=count/(size[0]*size[1])\n",
    "    return BER\n",
    "\n",
    "Pilot_file_name = 'Pilot_'+str(P)   # Here file name is \"Pilot_64\" (LJS)\n",
    "if os.path.isfile(Pilot_file_name):\n",
    "    print ('Load Training Pilots txt')\n",
    "    # load file\n",
    "    bits = np.loadtxt(Pilot_file_name, delimiter=',')\n",
    "else:\n",
    "    # write file\n",
    "    bits = np.random.binomial(n=1, p=0.5, size=(K*mu, ))\n",
    "    np.savetxt(Pilot_file_name, bits, delimiter=',')\n",
    "    \n",
    "pilotValue = Modulation(bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III : Deep learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Processing the train 1 th document\n",
      "Processing the train 2 th document\n",
      "Processing the train 3 th document\n",
      "Processing the train 4 th document\n",
      "Processing the train 5 th document\n",
      "Processing the train 6 th document\n",
      "Processing the train 7 th document\n",
      "Processing the train 8 th document\n",
      "Processing the train 9 th document\n",
      "Processing the train 10 th document\n",
      "Processing the train 11 th document\n",
      "Processing the train 12 th document\n",
      "Processing the train 13 th document\n",
      "Processing the train 14 th document\n",
      "Processing the train 15 th document\n",
      "Processing the train 16 th document\n",
      "Processing the train 17 th document\n",
      "Processing the train 18 th document\n",
      "Processing the train 19 th document\n",
      "Processing the train 20 th document\n",
      "Processing the train 21 th document\n",
      "Processing the train 22 th document\n",
      "Processing the train 23 th document\n",
      "Processing the train 24 th document\n",
      "Processing the train 25 th document\n",
      "Processing the train 26 th document\n",
      "Processing the train 27 th document\n",
      "Processing the train 28 th document\n",
      "Processing the train 29 th document\n",
      "Processing the train 30 th document\n",
      "Processing the train 31 th document\n",
      "Processing the train 32 th document\n",
      "Processing the train 33 th document\n",
      "Processing the train 34 th document\n",
      "Processing the train 35 th document\n",
      "Processing the train 36 th document\n",
      "Processing the train 37 th document\n",
      "Processing the train 38 th document\n",
      "Processing the train 39 th document\n",
      "Processing the train 40 th document\n",
      "Processing the train 41 th document\n",
      "Processing the train 42 th document\n",
      "Processing the train 43 th document\n",
      "Processing the train 44 th document\n",
      "Processing the train 45 th document\n",
      "Processing the train 46 th document\n",
      "Processing the train 47 th document\n",
      "Processing the train 48 th document\n",
      "Processing the train 49 th document\n",
      "Processing the train 50 th document\n",
      "Processing the train 51 th document\n",
      "Processing the train 52 th document\n",
      "Processing the train 53 th document\n",
      "Processing the train 54 th document\n",
      "Processing the train 55 th document\n",
      "Processing the train 56 th document\n",
      "Processing the train 57 th document\n",
      "Processing the train 58 th document\n",
      "Processing the train 59 th document\n",
      "Processing the train 60 th document\n",
      "Processing the train 61 th document\n",
      "Processing the train 62 th document\n",
      "Processing the train 63 th document\n",
      "Processing the train 64 th document\n",
      "Processing the train 65 th document\n",
      "Processing the train 66 th document\n",
      "Processing the train 67 th document\n",
      "Processing the train 68 th document\n",
      "Processing the train 69 th document\n",
      "Processing the train 70 th document\n",
      "Processing the train 71 th document\n",
      "Processing the train 72 th document\n",
      "Processing the train 73 th document\n",
      "Processing the train 74 th document\n",
      "Processing the train 75 th document\n",
      "Processing the train 76 th document\n",
      "Processing the train 77 th document\n",
      "Processing the train 78 th document\n",
      "Processing the train 79 th document\n",
      "Processing the train 80 th document\n",
      "Processing the train 81 th document\n",
      "Processing the train 82 th document\n",
      "Processing the train 83 th document\n",
      "Processing the train 84 th document\n",
      "Processing the train 85 th document\n",
      "Processing the train 86 th document\n",
      "Processing the train 87 th document\n",
      "Processing the train 88 th document\n",
      "Processing the train 89 th document\n",
      "Processing the train 90 th document\n",
      "Processing the train 91 th document\n",
      "Processing the train 92 th document\n",
      "Processing the train 93 th document\n",
      "Processing the train 94 th document\n",
      "Processing the train 95 th document\n",
      "Processing the train 96 th document\n",
      "Processing the train 97 th document\n",
      "Processing the train 98 th document\n",
      "Processing the train 99 th document\n",
      "Processing the train 100 th document\n",
      "Processing the train 101 th document\n",
      "Processing the train 102 th document\n",
      "Processing the train 103 th document\n",
      "Processing the train 104 th document\n",
      "Processing the train 105 th document\n",
      "Processing the train 106 th document\n",
      "Processing the train 107 th document\n",
      "Processing the train 108 th document\n",
      "Processing the train 109 th document\n",
      "Processing the train 110 th document\n",
      "Processing the train 111 th document\n",
      "Processing the train 112 th document\n",
      "Processing the train 113 th document\n",
      "Processing the train 114 th document\n",
      "Processing the train 115 th document\n",
      "Processing the train 116 th document\n",
      "Processing the train 117 th document\n",
      "Processing the train 118 th document\n",
      "Processing the train 119 th document\n",
      "Processing the train 120 th document\n",
      "Processing the train 121 th document\n",
      "Processing the train 122 th document\n",
      "Processing the train 123 th document\n",
      "Processing the train 124 th document\n",
      "Processing the train 125 th document\n",
      "Processing the train 126 th document\n",
      "Processing the train 127 th document\n",
      "Processing the train 128 th document\n",
      "Processing the train 129 th document\n",
      "Processing the train 130 th document\n",
      "Processing the train 131 th document\n",
      "Processing the train 132 th document\n",
      "Processing the train 133 th document\n",
      "Processing the train 134 th document\n",
      "Processing the train 135 th document\n",
      "Processing the train 136 th document\n",
      "Processing the train 137 th document\n",
      "Processing the train 138 th document\n",
      "Processing the train 139 th document\n",
      "Processing the train 140 th document\n",
      "Processing the train 141 th document\n",
      "Processing the train 142 th document\n",
      "Processing the train 143 th document\n",
      "Processing the train 144 th document\n",
      "Processing the train 145 th document\n",
      "Processing the train 146 th document\n",
      "Processing the train 147 th document\n",
      "Processing the train 148 th document\n",
      "Processing the train 149 th document\n",
      "Processing the train 150 th document\n",
      "Processing the train 151 th document\n",
      "Processing the train 152 th document\n",
      "Processing the train 153 th document\n",
      "Processing the train 154 th document\n",
      "Processing the train 155 th document\n",
      "Processing the train 156 th document\n",
      "Processing the train 157 th document\n",
      "Processing the train 158 th document\n",
      "Processing the train 159 th document\n",
      "Processing the train 160 th document\n",
      "Processing the train 161 th document\n",
      "Processing the train 162 th document\n",
      "Processing the train 163 th document\n",
      "Processing the train 164 th document\n",
      "Processing the train 165 th document\n",
      "Processing the train 166 th document\n",
      "Processing the train 167 th document\n",
      "Processing the train 168 th document\n",
      "Processing the train 169 th document\n",
      "Processing the train 170 th document\n",
      "Processing the train 171 th document\n",
      "Processing the train 172 th document\n",
      "Processing the train 173 th document\n",
      "Processing the train 174 th document\n",
      "Processing the train 175 th document\n",
      "Processing the train 176 th document\n",
      "Processing the train 177 th document\n",
      "Processing the train 178 th document\n",
      "Processing the train 179 th document\n",
      "Processing the train 180 th document\n",
      "Processing the train 181 th document\n",
      "Processing the train 182 th document\n",
      "Processing the train 183 th document\n",
      "Processing the train 184 th document\n",
      "Processing the train 185 th document\n",
      "Processing the train 186 th document\n",
      "Processing the train 187 th document\n",
      "Processing the train 188 th document\n",
      "Processing the train 189 th document\n",
      "Processing the train 190 th document\n",
      "Processing the train 191 th document\n",
      "Processing the train 192 th document\n",
      "Processing the train 193 th document\n",
      "Processing the train 194 th document\n",
      "Processing the train 195 th document\n",
      "Processing the train 196 th document\n",
      "Processing the train 197 th document\n",
      "Processing the train 198 th document\n",
      "Processing the train 199 th document\n",
      "Processing the train 200 th document\n",
      "Processing the train 201 th document\n",
      "Processing the train 202 th document\n",
      "Processing the train 203 th document\n",
      "Processing the train 204 th document\n",
      "Processing the train 205 th document\n",
      "Processing the train 206 th document\n",
      "Processing the train 207 th document\n",
      "Processing the train 208 th document\n",
      "Processing the train 209 th document\n",
      "Processing the train 210 th document\n",
      "Processing the train 211 th document\n",
      "Processing the train 212 th document\n",
      "Processing the train 213 th document\n",
      "Processing the train 214 th document\n",
      "Processing the train 215 th document\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the train 216 th document\n",
      "Processing the train 217 th document\n",
      "Processing the train 218 th document\n",
      "Processing the train 219 th document\n",
      "Processing the train 220 th document\n",
      "Processing the train 221 th document\n",
      "Processing the train 222 th document\n",
      "Processing the train 223 th document\n",
      "Processing the train 224 th document\n",
      "Processing the train 225 th document\n",
      "Processing the train 226 th document\n",
      "Processing the train 227 th document\n",
      "Processing the train 228 th document\n",
      "Processing the train 229 th document\n",
      "Processing the train 230 th document\n",
      "Processing the train 231 th document\n",
      "Processing the train 232 th document\n",
      "Processing the train 233 th document\n",
      "Processing the train 234 th document\n",
      "Processing the train 235 th document\n",
      "Processing the train 236 th document\n",
      "Processing the train 237 th document\n",
      "Processing the train 238 th document\n",
      "Processing the train 239 th document\n",
      "Processing the train 240 th document\n",
      "Processing the train 241 th document\n",
      "Processing the train 242 th document\n",
      "Processing the train 243 th document\n",
      "Processing the train 244 th document\n",
      "Processing the train 245 th document\n",
      "Processing the train 246 th document\n",
      "Processing the train 247 th document\n",
      "Processing the train 248 th document\n",
      "Processing the train 249 th document\n",
      "Processing the train 250 th document\n",
      "Processing the train 251 th document\n",
      "Processing the train 252 th document\n",
      "Processing the train 253 th document\n",
      "Processing the train 254 th document\n",
      "Processing the train 255 th document\n",
      "Processing the train 256 th document\n",
      "Processing the train 257 th document\n",
      "Processing the train 258 th document\n",
      "Processing the train 259 th document\n",
      "Processing the train 260 th document\n",
      "Processing the train 261 th document\n",
      "Processing the train 262 th document\n",
      "Processing the train 263 th document\n",
      "Processing the train 264 th document\n",
      "Processing the train 265 th document\n",
      "Processing the train 266 th document\n",
      "Processing the train 267 th document\n",
      "Processing the train 268 th document\n",
      "Processing the train 269 th document\n",
      "Processing the train 270 th document\n",
      "Processing the train 271 th document\n",
      "Processing the train 272 th document\n",
      "Processing the train 273 th document\n",
      "Processing the train 274 th document\n",
      "Processing the train 275 th document\n",
      "Processing the train 276 th document\n",
      "Processing the train 277 th document\n",
      "Processing the train 278 th document\n",
      "Processing the train 279 th document\n",
      "Processing the train 280 th document\n",
      "Processing the train 281 th document\n",
      "Processing the train 282 th document\n",
      "Processing the train 283 th document\n",
      "Processing the train 284 th document\n",
      "Processing the train 285 th document\n",
      "Processing the train 286 th document\n",
      "Processing the train 287 th document\n",
      "Processing the train 288 th document\n",
      "Processing the train 289 th document\n",
      "Processing the train 290 th document\n",
      "Processing the train 291 th document\n",
      "Processing the train 292 th document\n",
      "Processing the train 293 th document\n",
      "Processing the train 294 th document\n",
      "Processing the train 295 th document\n",
      "Processing the train 296 th document\n",
      "Processing the train 297 th document\n",
      "Processing the train 298 th document\n",
      "Processing the train 299 th document\n",
      "Processing the train 300 th document\n",
      "Processing the test 301 th document\n",
      "Processing the test 302 th document\n",
      "Processing the test 303 th document\n",
      "Processing the test 304 th document\n",
      "Processing the test 305 th document\n",
      "Processing the test 306 th document\n",
      "Processing the test 307 th document\n",
      "Processing the test 308 th document\n",
      "Processing the test 309 th document\n",
      "Processing the test 310 th document\n",
      "Processing the test 311 th document\n",
      "Processing the test 312 th document\n",
      "Processing the test 313 th document\n",
      "Processing the test 314 th document\n",
      "Processing the test 315 th document\n",
      "Processing the test 316 th document\n",
      "Processing the test 317 th document\n",
      "Processing the test 318 th document\n",
      "Processing the test 319 th document\n",
      "Processing the test 320 th document\n",
      "Processing the test 321 th document\n",
      "Processing the test 322 th document\n",
      "Processing the test 323 th document\n",
      "Processing the test 324 th document\n",
      "Processing the test 325 th document\n",
      "Processing the test 326 th document\n",
      "Processing the test 327 th document\n",
      "Processing the test 328 th document\n",
      "Processing the test 329 th document\n",
      "Processing the test 330 th document\n",
      "Processing the test 331 th document\n",
      "Processing the test 332 th document\n",
      "Processing the test 333 th document\n",
      "Processing the test 334 th document\n",
      "Processing the test 335 th document\n",
      "Processing the test 336 th document\n",
      "Processing the test 337 th document\n",
      "Processing the test 338 th document\n",
      "Processing the test 339 th document\n",
      "Processing the test 340 th document\n",
      "Processing the test 341 th document\n",
      "Processing the test 342 th document\n",
      "Processing the test 343 th document\n",
      "Processing the test 344 th document\n",
      "Processing the test 345 th document\n",
      "Processing the test 346 th document\n",
      "Processing the test 347 th document\n",
      "Processing the test 348 th document\n",
      "Processing the test 349 th document\n",
      "Processing the test 350 th document\n",
      "Processing the test 351 th document\n",
      "Processing the test 352 th document\n",
      "Processing the test 353 th document\n",
      "Processing the test 354 th document\n",
      "Processing the test 355 th document\n",
      "Processing the test 356 th document\n",
      "Processing the test 357 th document\n",
      "Processing the test 358 th document\n",
      "Processing the test 359 th document\n",
      "Processing the test 360 th document\n",
      "Processing the test 361 th document\n",
      "Processing the test 362 th document\n",
      "Processing the test 363 th document\n",
      "Processing the test 364 th document\n",
      "Processing the test 365 th document\n",
      "Processing the test 366 th document\n",
      "Processing the test 367 th document\n",
      "Processing the test 368 th document\n",
      "Processing the test 369 th document\n",
      "Processing the test 370 th document\n",
      "Processing the test 371 th document\n",
      "Processing the test 372 th document\n",
      "Processing the test 373 th document\n",
      "Processing the test 374 th document\n",
      "Processing the test 375 th document\n",
      "Processing the test 376 th document\n",
      "Processing the test 377 th document\n",
      "Processing the test 378 th document\n",
      "Processing the test 379 th document\n",
      "Processing the test 380 th document\n",
      "Processing the test 381 th document\n",
      "Processing the test 382 th document\n",
      "Processing the test 383 th document\n",
      "Processing the test 384 th document\n",
      "Processing the test 385 th document\n",
      "Processing the test 386 th document\n",
      "Processing the test 387 th document\n",
      "Processing the test 388 th document\n",
      "Processing the test 389 th document\n",
      "Processing the test 390 th document\n",
      "Processing the test 391 th document\n",
      "Processing the test 392 th document\n",
      "Processing the test 393 th document\n",
      "Processing the test 394 th document\n",
      "Processing the test 395 th document\n",
      "Processing the test 396 th document\n",
      "Processing the test 397 th document\n",
      "Processing the test 398 th document\n",
      "Processing the test 399 th document\n",
      "Processing the test 400 th document\n",
      "length of training channel response is 3000000\n",
      "length of testing channel response is 1000000\n",
      "shape of training channel response is (3000000, 16)\n",
      "shape of testing channel response is (1000000, 16)\n",
      "========================================\n",
      "Processing the 1 th epoch\n",
      "epoch: 0001\n",
      "cost= 0.252580745\n",
      "This is a Big Test Set \n",
      "WARNING:tensorflow:From <timed exec>:151: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "OFDM Detection QAM output number is 16\n",
      "SNR =  20\n",
      "Num Pilot 64\n",
      "prediction and the mean error on test set are: 0.50005007 0.50126874\n",
      "BER : 0.50126875\n",
      "Prediction and the mean error on train set are: 0.5013106 0.50450003\n",
      "========================================\n",
      "Processing the 2 th epoch\n",
      "========================================\n",
      "Processing the 3 th epoch\n",
      "========================================\n",
      "Processing the 4 th epoch\n",
      "========================================\n",
      "Processing the 5 th epoch\n",
      "========================================\n",
      "Processing the 6 th epoch\n",
      "========================================\n",
      "Processing the 7 th epoch\n",
      "========================================\n",
      "Processing the 8 th epoch\n",
      "========================================\n",
      "Processing the 9 th epoch\n",
      "========================================\n",
      "Processing the 10 th epoch\n",
      "========================================\n",
      "Processing the 11 th epoch\n",
      "========================================\n",
      "Processing the 12 th epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Processing the 13 th epoch\n",
      "========================================\n",
      "Processing the 14 th epoch\n",
      "========================================\n",
      "Processing the 15 th epoch\n",
      "========================================\n",
      "Processing the 16 th epoch\n",
      "========================================\n",
      "Processing the 17 th epoch\n",
      "========================================\n",
      "Processing the 18 th epoch\n",
      "========================================\n",
      "Processing the 19 th epoch\n",
      "========================================\n",
      "Processing the 20 th epoch\n",
      "========================================\n",
      "Processing the 21 th epoch\n",
      "========================================\n",
      "Processing the 22 th epoch\n",
      "========================================\n",
      "Processing the 23 th epoch\n",
      "========================================\n",
      "Processing the 24 th epoch\n",
      "========================================\n",
      "Processing the 25 th epoch\n",
      "========================================\n",
      "Processing the 26 th epoch\n",
      "========================================\n",
      "Processing the 27 th epoch\n",
      "========================================\n",
      "Processing the 28 th epoch\n",
      "========================================\n",
      "Processing the 29 th epoch\n",
      "========================================\n",
      "Processing the 30 th epoch\n",
      "========================================\n",
      "Processing the 31 th epoch\n",
      "========================================\n",
      "Processing the 32 th epoch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-26fa829aa7b7>\u001b[0m in \u001b[0;36mofdm_simulate\u001b[0;34m(codeword, channelResponse, SNRdb)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mOFDM_withCP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddCP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOFDM_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mOFDM_TX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOFDM_withCP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mOFDM_RX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOFDM_TX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannelResponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSNRdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mOFDM_RX_noCP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveCP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOFDM_RX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m#     plt.plot(np.abs(OFDM_RX_noCP))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-26fa829aa7b7>\u001b[0m in \u001b[0;36mchannel\u001b[0;34m(signal, channelResponse, SNRdb)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m  \u001b[0mprob\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mpower1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mpower1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training parameters\n",
    "# training_epochs = 20\n",
    "# batch_size = 256\n",
    "display_step = 100  #5\n",
    "test_step = 200\n",
    "cost_step = 25  # LJS\n",
    "# examples_to_show = 10  \n",
    "# Network Parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 250 # 1st layer num features\n",
    "n_hidden_3 = 120 # 2nd layer num features\n",
    "n_input = 256  \n",
    "n_output = 16 # every 16 bit are predicted by a model\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "weights = {                    \n",
    "    'encoder_h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],stddev=0.1)),\n",
    "    'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],stddev=0.1)),\n",
    "    'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3],stddev=0.1)),\n",
    "    'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_output],stddev=0.1)),            \n",
    "}\n",
    "biases = {            \n",
    "    'encoder_b1': tf.Variable(tf.truncated_normal([n_hidden_1],stddev=0.1)),\n",
    "    'encoder_b2': tf.Variable(tf.truncated_normal([n_hidden_2],stddev=0.1)),\n",
    "    'encoder_b3': tf.Variable(tf.truncated_normal([n_hidden_3],stddev=0.1)),\n",
    "    'encoder_b4': tf.Variable(tf.truncated_normal([n_output],stddev=0.1)),          \n",
    "\n",
    "}\n",
    "\n",
    "# Encoder Hidden layer with sigmoid activation # 1\n",
    "# layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['encoder_h1']), biases['encoder_b1']))\n",
    "layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['encoder_h3']), biases['encoder_b3']))\n",
    "layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['encoder_h4']), biases['encoder_b4']))\n",
    "       \n",
    "y_pred = layer_4\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = Y\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start Training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# The H information set\n",
    "H_folder_train = '../H_dataset/train/'\n",
    "H_folder_test = '../H_dataset/test/'\n",
    "train_idx_low = 1\n",
    "train_idx_high = 301\n",
    "test_idx_low = 301\n",
    "test_idx_high = 401\n",
    "\n",
    "# =================== Saving Channel conditions to a large matrix ================ #\n",
    "channel_response_set_train = []\n",
    "for train_idx in range(train_idx_low,train_idx_high):\n",
    "    print(\"Processing the train\", train_idx, \"th document\")\n",
    "    H_file = H_folder_train + str(train_idx) + '.txt'\n",
    "    with open(H_file) as f:\n",
    "        for line in f:\n",
    "            numbers_str = line.split()\n",
    "            # np.shape(numbers_str)=32 x 1\n",
    "            numbers_float = [float(x) for x in numbers_str]\n",
    "            # np.shape(numbers_float)=32 x 1\n",
    "            h_response = np.asarray(numbers_float[0:int(len(numbers_float)/2)])+\\\n",
    "            1j*np.asarray(numbers_float[int(len(numbers_float)/2):len(numbers_float)])\n",
    "            channel_response_set_train.append(h_response)\n",
    "        \n",
    "channel_response_set_test = []\n",
    "for test_idx in range(test_idx_low,test_idx_high):\n",
    "    print(\"Processing the test\", test_idx, \"th document\")\n",
    "    H_file = H_folder_test + str(test_idx) + '.txt'\n",
    "    with open(H_file) as f:\n",
    "        for line in f:\n",
    "            numbers_str = line.split()\n",
    "            numbers_float = [float(x) for x in numbers_str]\n",
    "            h_response = np.asarray(numbers_float[0:int(len(numbers_float)/2)])+\\\n",
    "            1j*np.asarray(numbers_float[int(len(numbers_float)/2):len(numbers_float)])\n",
    "            channel_response_set_test.append(h_response)\n",
    "            # print(np.shape(channel_response_set_test))\n",
    "\n",
    "print ('length of training channel response is', len(channel_response_set_train))\n",
    "print ('length of testing channel response is', len(channel_response_set_test))\n",
    "print ('shape of training channel response is', np.shape(channel_response_set_train))\n",
    "print ('shape of testing channel response is', np.shape(channel_response_set_test))\n",
    "#=================== end ================ #\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    training_epochs = 5000 #20000\n",
    "    learning_rate_current = 0.001   #0.01\n",
    "    training_cost=np.zeros([1,200]) # LJS\n",
    "    for epoch in range(training_epochs):\n",
    "        print (\"========================================\")\n",
    "        print('Processing the',epoch+1,'th epoch')\n",
    "        if epoch > 0 and epoch % 500 == 0: # 2000\n",
    "            learning_rate_current = learning_rate_current/5                    \n",
    "        avg_cost = 0.0\n",
    "        total_batch = 50   #50        \n",
    "\n",
    "        for index_m in range(total_batch):\n",
    "            input_samples = []\n",
    "            input_labels = []\n",
    "            for index_k in range(0, 500):     #1000\n",
    "                bits = np.random.binomial(n=1, p=0.5, size=(payloadBits_per_OFDM, ))\n",
    "                channel_response = channel_response_set_train[np.random.randint(0,len(channel_response_set_train))]  \n",
    "                signal_output, para = ofdm_simulate(bits,channel_response,SNRdb)   \n",
    "                input_labels.append(bits[16:32])    \n",
    "                input_samples.append(signal_output)\n",
    "            batch_x = np.asarray(input_samples)\n",
    "            batch_y = np.asarray(input_labels)\n",
    "            _,c = sess.run([optimizer,cost], feed_dict={X:batch_x,# input\n",
    "                                                        Y:batch_y,# labels\n",
    "                                                        learning_rate:learning_rate_current})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if epoch % cost_step == 0 :\n",
    "            training_cost[0,epoch//cost_step] = avg_cost  \n",
    "\n",
    "        if epoch % display_step == 0:  # == 0\n",
    "            print(\"epoch:\",'%04d' % (epoch+1))\n",
    "            print(\"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            #print (\"========================================\")\n",
    "            input_samples_test = []\n",
    "            input_labels_test = []\n",
    "            test_number = 1000  #1000\n",
    "            # set test channel response for this epoch                    \n",
    "            if epoch % test_step == 0: \n",
    "                #print (\"========================================\")\n",
    "                print (\"This is a Big Test Set \")\n",
    "                test_number = 10000  # 10000\n",
    "            for i in range(0, test_number):\n",
    "                bits = np.random.binomial(n=1, p=0.5, size=(payloadBits_per_OFDM, ))  \n",
    "                channel_response= channel_response_set_test[np.random.randint(0,len(channel_response_set_test))]\n",
    "                signal_output, para = ofdm_simulate(bits,channel_response,SNRdb)\n",
    "                input_labels_test.append(bits[16:32])\n",
    "                input_samples_test.append(signal_output)\n",
    "\n",
    "            batch_x = np.asarray(input_samples_test)\n",
    "            batch_y = np.asarray(input_labels_test)\n",
    "            # encode_decode = sess.run(y_pred, feed_dict = {X:batch_x})\n",
    "            mean_error = tf.reduce_mean(abs(y_pred - batch_y))\n",
    "            mean_error_rate = 1-tf.reduce_mean(tf.reduce_mean(tf.to_float(tf.equal(tf.sign(y_pred-0.5), tf.cast(tf.sign(batch_y-0.5),tf.float32))),1))               \n",
    "            # print(\"OFDM Detection QAM output number is\", n_output, \",SNR = \", SNRdb, \",Num Pilot = \", P,\", prediction and the mean error on test set are:\", mean_error.eval({X:batch_x}), mean_error_rate.eval({X:batch_x}))\n",
    "            print(\"OFDM Detection QAM output number is\", n_output)\n",
    "            print(\"SNR = \", SNRdb)\n",
    "            print(\"Num Pilot\", P)\n",
    "            print(\"prediction and the mean error on test set are:\", \n",
    "                  mean_error.eval({X:batch_x}), mean_error_rate.eval({X:batch_x}))\n",
    "            y_pred_np=y_pred.eval({X:batch_x})  # LJS\n",
    "            BER=decision(y_pred_np,batch_y)     # LJS\n",
    "            print('BER :',BER)                  # LJS\n",
    "\n",
    "            batch_x = np.asarray(input_samples)\n",
    "            batch_y = np.asarray(input_labels)\n",
    "            # encode_decode = sess.run(y_pred, feed_dict = {X:batch_x})\n",
    "            mean_error = tf.reduce_mean(abs(y_pred - batch_y))                    \n",
    "            mean_error_rate = 1-tf.reduce_mean(tf.reduce_mean(tf.to_float(tf.equal(tf.sign(y_pred-0.5), tf.cast(tf.sign(batch_y-0.5),tf.float32))),1))\n",
    "            print(\"Prediction and the mean error on train set are:\",\\\n",
    "                  mean_error.eval({X:batch_x}), mean_error_rate.eval({X:batch_x}))\n",
    "            #print (\"========================================\")\n",
    "    print(\"Total epochs cost : \\n\",training_cost)  # LJS\n",
    "    print(\"optimization finished\")\n",
    "    # save the model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"net_AWGN_thesis/epochs_5000.ckpt\")\n",
    "    print(\"Save to path: \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV : Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda_python3.7_installfolder\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from net_BG_cost/epochs_5000.ckpt\n",
      "SNR =  5 BER : 0.2147625\n",
      "SNR =  6 BER : 0.20523125\n",
      "SNR =  7 BER : 0.19449375\n",
      "SNR =  8 BER : 0.18785\n",
      "SNR =  9 BER : 0.17891875\n",
      "SNR =  10 BER : 0.178025\n",
      "SNR =  11 BER : 0.1693625\n",
      "SNR =  12 BER : 0.17008125\n",
      "SNR =  13 BER : 0.1618125\n",
      "SNR =  14 BER : 0.16109375\n",
      "SNR =  15 BER : 0.16214375\n",
      "SNR =  16 BER : 0.1587625\n",
      "SNR =  17 BER : 0.1559\n",
      "SNR =  18 BER : 0.1551875\n",
      "SNR =  19 BER : 0.15753125\n",
      "SNR =  20 BER : 0.15656875\n",
      "SNR =  21 BER : 0.151075\n",
      "SNR =  22 BER : 0.15456875\n",
      "SNR =  23 BER : 0.15395625\n",
      "SNR =  24 BER : 0.15518125\n",
      "SNR =  25 BER : 0.15276875\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"net_AWGN_thesis/epochs_5000.ckpt\") \n",
    "    for k in range(5,31): # (5,26)\n",
    "        SNRdB=k   # SNRdB=k\n",
    "        input_samples_test = []\n",
    "        input_labels_test = []\n",
    "        test_number = 10000         # 1000 \n",
    "        for i in range(0, test_number):\n",
    "            bits = np.random.binomial(n=1, p=0.5, size=(payloadBits_per_OFDM, ))   \n",
    "            channel_response= channel_response_set_test[np.random.randint(0,len(channel_response_set_test))]\n",
    "            signal_output, para = ofdm_simulate(bits,channel_response,SNRdB)\n",
    "            input_labels_test.append(bits[16:32])\n",
    "            input_samples_test.append(signal_output)\n",
    "\n",
    "        batch_x = np.asarray(input_samples_test)\n",
    "        batch_y = np.asarray(input_labels_test)\n",
    "        y_pred_np=y_pred.eval({X:batch_x})            # LJS\n",
    "        BER=decision(y_pred_np,batch_y)               # LJS\n",
    "        print('SNR = ',SNRdB,'BER :',BER)             # LJS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------Programs end-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
